{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a57bkuBlT3mQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "import ipaddress\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fraud_Data, Credit Card Data, and IP-to-Country data\n",
        "fraud_df = pd.read_csv(\"/content/Fraud_Data.csv\")\n",
        "creditcard_df = pd.read_csv(\"/content/creditcard1.csv\")\n",
        "ip_country_df = pd.read_csv(\"/content/IpAddress_to_Country.csv\")\n"
      ],
      "metadata": {
        "id": "_yVs8ldKcHe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 3: Data Cleaning - IP Address to Country Data\n",
        "# ============================================================\n",
        "\n",
        "# Convert lower_bound_ip_address to integer (it was a float)\n",
        "ip_country_df['lower_bound_ip_address'] = ip_country_df['lower_bound_ip_address'].astype(int)\n"
      ],
      "metadata": {
        "id": "WktFhMraejhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 4: Data Cleaning - Fraud Data\n",
        "# ============================================================\n",
        "\n",
        "# Remove duplicate rows from fraud_df (if any)\n",
        "fraud_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Check for missing values and print a summary\n",
        "print(\"Missing values in Fraud Data:\")\n",
        "print(fraud_df.isna().sum())\n",
        "\n",
        "# Convert timestamp columns to datetime objects.\n",
        "# (Assuming columns are named 'signup_time' and 'purchase_time')\n",
        "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'], errors='coerce')\n",
        "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'], errors='coerce')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dOLcSjXe2LT",
        "outputId": "61979c88-27e2-40af-bb0f-87fa2ec258c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in Fraud Data:\n",
            "user_id           0\n",
            "signup_time       0\n",
            "purchase_time     0\n",
            "purchase_value    0\n",
            "device_id         0\n",
            "source            0\n",
            "browser           0\n",
            "sex               0\n",
            "age               0\n",
            "ip_address        0\n",
            "class             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 5: Convert IP Address Strings to Integer\n",
        "# ============================================================\n",
        "\n",
        "# Define a function that converts an IPv4 address (string) to an integer.\n",
        "def ip_to_int(ip_str):\n",
        "    try:\n",
        "        return int(ipaddress.IPv4Address(ip_str))\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# Apply the conversion to create a new column 'ip_int'\n",
        "fraud_df['ip_int'] = fraud_df['ip_address'].apply(ip_to_int)\n"
      ],
      "metadata": {
        "id": "Jz70z3Ubf_WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 6: Merge Fraud Data with IP-to-Country Data\n",
        "# ============================================================\n",
        "\n",
        "# Define a function that maps an IP (as an integer) to a country using the IP ranges\n",
        "def map_ip_to_country(ip_int):\n",
        "    # Find the row in ip_country_df where ip_int falls between the lower and upper bounds\n",
        "    row = ip_country_df[(ip_country_df['lower_bound_ip_address'] <= ip_int) &\n",
        "                        (ip_country_df['upper_bound_ip_address'] >= ip_int)]\n",
        "    if not row.empty:\n",
        "        return row.iloc[0]['country']\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "# Create a new column 'country' in fraud_df by applying the mapping function\n",
        "fraud_df['country'] = fraud_df['ip_int'].apply(map_ip_to_country)\n",
        "\n",
        "# Save the merged DataFrame to a CSV file named 'merged_ip.csv'\n",
        "fraud_df.to_csv(\"merged_ip.csv\", index=False)\n",
        "\n",
        "print(\"Merged file saved as merged_ip.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eo9kSxQgH-Q",
        "outputId": "6ce77af1-30a3-4289-c7ea-e72630ebb9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged file saved as merged_ip.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# STEP 7: Feature Engineering - Time-Based Features\n",
        "# ============================================================\n",
        "\n",
        "# Extract the hour of day and day of week from the purchase_time column\n",
        "fraud_df['purchase_hour'] = fraud_df['purchase_time'].dt.hour\n",
        "fraud_df['purchase_dayofweek'] = fraud_df['purchase_time'].dt.dayofweek\n"
      ],
      "metadata": {
        "id": "9QSqI_y9gMU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 8: Normalize a Key Feature (purchase_value)\n",
        "# ============================================================\n",
        "\n",
        "# Using MinMaxScaler to normalize the 'purchase_value' column (assumed to be the purchase amount)\n",
        "scaler = MinMaxScaler()\n",
        "# Reshape is required because scaler expects a 2D array\n",
        "fraud_df['purchase_value_scaled'] = scaler.fit_transform(fraud_df[['purchase_value']])\n"
      ],
      "metadata": {
        "id": "6iv_-HTDhVED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 9: Exploratory Data Analysis (EDA) - Quick Look\n",
        "# ============================================================\n",
        "\n",
        "# Print a summary of the fraud dataset to inspect data types and new columns\n",
        "print(\"\\nFraud Data Info:\")\n",
        "print(fraud_df.info())\n",
        "\n",
        "# Display the first few rows to verify the changes\n",
        "print(\"\\nFraud Data Sample:\")\n",
        "print(fraud_df.head())\n",
        "\n",
        "# Optionally, you can also print summary statistics\n",
        "print(\"\\nFraud Data Summary Statistics:\")\n",
        "print(fraud_df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GCfbj82haJv",
        "outputId": "9b652e31-732d-4f01-f884-0ccf19d7eea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fraud Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 151112 entries, 0 to 151111\n",
            "Data columns (total 16 columns):\n",
            " #   Column                 Non-Null Count   Dtype         \n",
            "---  ------                 --------------   -----         \n",
            " 0   user_id                151112 non-null  int64         \n",
            " 1   signup_time            151112 non-null  datetime64[ns]\n",
            " 2   purchase_time          151112 non-null  datetime64[ns]\n",
            " 3   purchase_value         151112 non-null  int64         \n",
            " 4   device_id              151112 non-null  object        \n",
            " 5   source                 151112 non-null  object        \n",
            " 6   browser                151112 non-null  object        \n",
            " 7   sex                    151112 non-null  object        \n",
            " 8   age                    151112 non-null  int64         \n",
            " 9   ip_address             151112 non-null  float64       \n",
            " 10  class                  151112 non-null  int64         \n",
            " 11  ip_int                 0 non-null       float64       \n",
            " 12  country                0 non-null       float64       \n",
            " 13  purchase_hour          151112 non-null  int32         \n",
            " 14  purchase_dayofweek     151112 non-null  int32         \n",
            " 15  purchase_value_scaled  151112 non-null  float64       \n",
            "dtypes: datetime64[ns](2), float64(4), int32(2), int64(4), object(4)\n",
            "memory usage: 17.3+ MB\n",
            "None\n",
            "\n",
            "Fraud Data Sample:\n",
            "   user_id         signup_time       purchase_time  purchase_value  \\\n",
            "0    22058 2015-02-24 22:55:49 2015-04-18 02:47:11              34   \n",
            "1   333320 2015-06-07 20:39:50 2015-06-08 01:38:54              16   \n",
            "2     1359 2015-01-01 18:52:44 2015-01-01 18:52:45              15   \n",
            "3   150084 2015-04-28 21:13:25 2015-05-04 13:54:50              44   \n",
            "4   221365 2015-07-21 07:09:52 2015-09-09 18:40:53              39   \n",
            "\n",
            "       device_id source browser sex  age    ip_address  class  ip_int  \\\n",
            "0  QVPSPJUOCKZAR    SEO  Chrome   M   39  7.327584e+08      0     NaN   \n",
            "1  EOGFQPIZPYXFZ    Ads  Chrome   F   53  3.503114e+08      0     NaN   \n",
            "2  YSSKYOSJHPPLJ    SEO   Opera   M   53  2.621474e+09      1     NaN   \n",
            "3  ATGTXKYKUDUQN    SEO  Safari   M   41  3.840542e+09      0     NaN   \n",
            "4  NAUITBZFJKHWW    Ads  Safari   M   45  4.155831e+08      0     NaN   \n",
            "\n",
            "   country  purchase_hour  purchase_dayofweek  purchase_value_scaled  \n",
            "0      NaN              2                   5               0.172414  \n",
            "1      NaN              1                   0               0.048276  \n",
            "2      NaN             18                   3               0.041379  \n",
            "3      NaN             13                   0               0.241379  \n",
            "4      NaN             18                   2               0.206897  \n",
            "\n",
            "Fraud Data Summary Statistics:\n",
            "             user_id                    signup_time  \\\n",
            "count  151112.000000                         151112   \n",
            "mean   200171.040970  2015-04-20 00:56:09.511329280   \n",
            "min         2.000000            2015-01-01 00:00:42   \n",
            "25%    100642.500000     2015-02-18 09:52:48.500000   \n",
            "50%    199958.000000            2015-04-19 04:41:30   \n",
            "75%    300054.000000  2015-06-18 14:47:22.750000128   \n",
            "max    400000.000000            2015-08-18 04:40:29   \n",
            "std    115369.285024                            NaN   \n",
            "\n",
            "                       purchase_time  purchase_value            age  \\\n",
            "count                         151112   151112.000000  151112.000000   \n",
            "mean   2015-06-16 02:56:38.759952896       36.935372      33.140704   \n",
            "min              2015-01-01 00:00:44        9.000000      18.000000   \n",
            "25%       2015-04-18 14:41:25.500000       22.000000      27.000000   \n",
            "50%       2015-06-18 13:46:17.500000       35.000000      33.000000   \n",
            "75%       2015-08-17 18:48:31.500000       49.000000      39.000000   \n",
            "max              2015-12-16 02:56:05      154.000000      76.000000   \n",
            "std                              NaN       18.322762       8.617733   \n",
            "\n",
            "         ip_address          class  ip_int  country  purchase_hour  \\\n",
            "count  1.511120e+05  151112.000000     0.0      0.0  151112.000000   \n",
            "mean   2.152145e+09       0.093646     NaN      NaN      11.521593   \n",
            "min    5.209350e+04       0.000000     NaN      NaN       0.000000   \n",
            "25%    1.085934e+09       0.000000     NaN      NaN       6.000000   \n",
            "50%    2.154770e+09       0.000000     NaN      NaN      12.000000   \n",
            "75%    3.243258e+09       0.000000     NaN      NaN      17.000000   \n",
            "max    4.294850e+09       1.000000     NaN      NaN      23.000000   \n",
            "std    1.248497e+09       0.291336     NaN      NaN       6.912474   \n",
            "\n",
            "       purchase_dayofweek  purchase_value_scaled  \n",
            "count       151112.000000          151112.000000  \n",
            "mean             3.011819               0.192658  \n",
            "min              0.000000               0.000000  \n",
            "25%              1.000000               0.089655  \n",
            "50%              3.000000               0.179310  \n",
            "75%              5.000000               0.275862  \n",
            "max              6.000000               1.000000  \n",
            "std              2.006203               0.126364  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 10: (Optional) Quick EDA on Credit Card Data\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nCredit Card Data Info:\")\n",
        "print(creditcard_df.info())\n",
        "print(\"\\nCredit Card Data Sample:\")\n",
        "print(creditcard_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rc9wYvvhjzN",
        "outputId": "c805a359-46f7-41d0-95ca-ff7bd49d09d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Credit Card Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7973 entries, 0 to 7972\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Time    7973 non-null   int64  \n",
            " 1   V1      7973 non-null   float64\n",
            " 2   V2      7973 non-null   float64\n",
            " 3   V3      7973 non-null   float64\n",
            " 4   V4      7973 non-null   float64\n",
            " 5   V5      7973 non-null   float64\n",
            " 6   V6      7973 non-null   float64\n",
            " 7   V7      7973 non-null   float64\n",
            " 8   V8      7973 non-null   float64\n",
            " 9   V9      7973 non-null   float64\n",
            " 10  V10     7973 non-null   float64\n",
            " 11  V11     7973 non-null   float64\n",
            " 12  V12     7973 non-null   float64\n",
            " 13  V13     7973 non-null   float64\n",
            " 14  V14     7973 non-null   float64\n",
            " 15  V15     7972 non-null   float64\n",
            " 16  V16     7972 non-null   float64\n",
            " 17  V17     7972 non-null   float64\n",
            " 18  V18     7972 non-null   float64\n",
            " 19  V19     7972 non-null   float64\n",
            " 20  V20     7972 non-null   float64\n",
            " 21  V21     7972 non-null   float64\n",
            " 22  V22     7972 non-null   float64\n",
            " 23  V23     7972 non-null   float64\n",
            " 24  V24     7972 non-null   float64\n",
            " 25  V25     7972 non-null   float64\n",
            " 26  V26     7972 non-null   float64\n",
            " 27  V27     7972 non-null   float64\n",
            " 28  V28     7972 non-null   float64\n",
            " 29  Amount  7972 non-null   float64\n",
            " 30  Class   7972 non-null   float64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 1.9 MB\n",
            "None\n",
            "\n",
            "Credit Card Data Sample:\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
            "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
            "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
            "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned and merged Fraud Data with IP-to-Country information\n",
        "fraud_df.to_csv(\"cleaned_fraud_data.csv\", index=False)\n",
        "print(\"Cleaned fraud data saved as 'cleaned_fraud_data.csv'.\")\n",
        "\n",
        "# Save the cleaned Credit Card Data as well\n",
        "creditcard_df.to_csv(\"cleaned_creditcard_data.csv\", index=False)\n",
        "print(\"Cleaned credit card data saved as 'cleaned_creditcard_data.csv'.\")\n",
        "\n",
        "# (Optional) Save the cleaned IP-to-Country data if needed\n",
        "ip_country_df.to_csv(\"cleaned_ip_country_data.csv\", index=False)\n",
        "print(\"Cleaned IP-to-Country data saved as 'cleaned_ip_country_data.csv'.\")\n"
      ],
      "metadata": {
        "id": "wo8fWxcTiP6D",
        "outputId": "fed991d7-9120-4f90-cb4e-f4a800eac4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned fraud data saved as 'cleaned_fraud_data.csv'.\n",
            "Cleaned credit card data saved as 'cleaned_creditcard_data.csv'.\n",
            "Cleaned IP-to-Country data saved as 'cleaned_ip_country_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the cleaned credit card dataset\n",
        "df_cc = pd.read_csv(r\"/content/cleaned_creditcard_data.csv\")\n",
        "\n",
        "# Count rows with NaN in any column\n",
        "total_rows_with_nan = df_cc.isna().any(axis=1).sum()\n",
        "\n",
        "print(f\"Total rows with NaN values in any column: {total_rows_with_nan}\")\n",
        "\n",
        "# Count rows with NaN in specific column(s)\n",
        "rows_with_nan_in_class = df_cc['Class'].isna().sum()\n",
        "print(f\"Rows with NaN in 'Class' column: {rows_with_nan_in_class}\")\n",
        "\n",
        "# You can also list columns with NaN and their count\n",
        "nan_counts_per_column = df_cc.isna().sum()\n",
        "print(\"\\nNaN counts per column:\")\n",
        "print(nan_counts_per_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBEVQxcszEaD",
        "outputId": "b1a533e8-abee-4098-ee96-f07ff84bca92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows with NaN values in any column: 1\n",
            "Rows with NaN in 'Class' column: 1\n",
            "\n",
            "NaN counts per column:\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       1\n",
            "V16       1\n",
            "V17       1\n",
            "V18       1\n",
            "V19       1\n",
            "V20       1\n",
            "V21       1\n",
            "V22       1\n",
            "V23       1\n",
            "V24       1\n",
            "V25       1\n",
            "V26       1\n",
            "V27       1\n",
            "V28       1\n",
            "Amount    1\n",
            "Class     1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer  # Import SimpleImputer\n",
        "\n",
        "# Load the cleaned credit card dataset\n",
        "df_cc = pd.read_csv(r\"/content/cleaned_creditcard_data.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_cc.drop('Class', axis=1)\n",
        "y = df_cc['Class']\n",
        "\n",
        "# Handle NaN values in 'y' (if any)\n",
        "y.fillna(y.mode()[0], inplace=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Impute NaN values in X_train and X_test using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Scale the features after imputation\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)  # Use imputed data\n",
        "X_test_scaled = scaler.transform(X_test_imputed)      # Use imputed data"
      ],
      "metadata": {
        "id": "noMBW__owYlf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESJkIIerz7pr",
        "outputId": "9897707f-93be-4e25-c2d7-71865a28db5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.20.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting mlflow-skinny==2.20.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.20.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.37)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.20.1->mlflow)\n",
            "  Downloading databricks_sdk-0.43.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (1.16.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (4.25.6)\n",
            "Requirement already satisfied: pydantic<3,>=1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (2.10.6)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (4.12.2)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.1->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (75.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (0.37b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.0->mlflow-skinny==2.20.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.0->mlflow-skinny==2.20.1->mlflow) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.1->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.1->mlflow) (2025.1.31)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.20.1-py3-none-any.whl (28.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.3/28.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.20.1-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.43.0-py3-none-any.whl (647 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.4/647.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, gunicorn, graphql-core, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 databricks-sdk-0.43.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.20.1 mlflow-skinny-2.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define a dictionary of models to train\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Set an MLflow experiment name (this creates or uses an existing experiment)\n",
        "mlflow.set_experiment(\"CreditCard_Fraud_Detection\")\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Train the model\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        # Calculate accuracy and print a classification report\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{model_name} Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Log parameters and metrics to MLflow\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_metric(\"accuracy\", acc)\n",
        "        mlflow.sklearn.log_model(model, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guAcg-VDz2mh",
        "outputId": "27cc6333-e849-43ba-c7ca-bedcbfd7f95e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression Accuracy: 0.9994\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1590\n",
            "         1.0       0.83      1.00      0.91         5\n",
            "\n",
            "    accuracy                           1.00      1595\n",
            "   macro avg       0.92      1.00      0.95      1595\n",
            "weighted avg       1.00      1.00      1.00      1595\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/02/10 12:27:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree Accuracy: 0.9994\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1590\n",
            "         1.0       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           1.00      1595\n",
            "   macro avg       1.00      0.90      0.94      1595\n",
            "weighted avg       1.00      1.00      1.00      1595\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/02/10 12:27:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest Accuracy: 0.9994\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1590\n",
            "         1.0       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           1.00      1595\n",
            "   macro avg       1.00      0.90      0.94      1595\n",
            "weighted avg       1.00      1.00      1.00      1595\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/02/10 12:27:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoosting Accuracy: 0.9994\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      1590\n",
            "         1.0       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           1.00      1595\n",
            "   macro avg       1.00      0.90      0.94      1595\n",
            "weighted avg       1.00      1.00      1.00      1595\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/02/10 12:28:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build a simple MLP model\n",
        "mlp_model = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "mlp_history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "loss, accuracy = mlp_model.evaluate(X_test_scaled, y_test)\n",
        "print(\"MLP Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc-kDNUc1Wb4",
        "outputId": "34acd5f7-d856-4caa-b055-ad4619b5a837"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7902 - loss: 0.4246 - val_accuracy: 0.9984 - val_loss: 0.0193\n",
            "Epoch 2/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0454 - val_accuracy: 0.9984 - val_loss: 0.0057\n",
            "Epoch 3/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9981 - loss: 0.0198 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 4/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9981 - loss: 0.0130 - val_accuracy: 1.0000 - val_loss: 6.2069e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9988 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 5.9896e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0068 - val_accuracy: 0.9992 - val_loss: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9990 - loss: 0.0093 - val_accuracy: 0.9992 - val_loss: 9.7820e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 0.9992 - val_loss: 0.0012\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0027    \n",
            "MLP Test Accuracy: 0.9993730187416077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# Reshape data: (samples, timesteps, channels)\n",
        "X_train_cnn = np.expand_dims(X_train_scaled, axis=2)\n",
        "X_test_cnn = np.expand_dims(X_test_scaled, axis=2)\n",
        "\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_history = cnn_model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "loss, accuracy = cnn_model.evaluate(X_test_cnn, y_test)\n",
        "print(\"CNN Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgoTSt8k6kdp",
        "outputId": "7a01958e-bc00-41a5-f718-daee6f8540ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.1152 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
            "Epoch 2/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9975 - loss: 0.0154 - val_accuracy: 0.9992 - val_loss: 0.0016\n",
            "Epoch 3/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 0.9992 - val_loss: 0.0031\n",
            "Epoch 4/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.9992 - val_loss: 0.0016\n",
            "Epoch 5/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 4.1795e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 7.6096e-04 - val_accuracy: 0.9992 - val_loss: 0.0026\n",
            "Epoch 7/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9992 - val_loss: 9.5852e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 9.9404e-04 - val_accuracy: 1.0000 - val_loss: 4.6060e-04\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0024    \n",
            "CNN Test Accuracy: 0.9993730187416077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, LSTM\n",
        "\n",
        "# Simple RNN Model\n",
        "rnn_model = Sequential([\n",
        "    SimpleRNN(32, input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "rnn_history = rnn_model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "loss, accuracy = rnn_model.evaluate(X_test_cnn, y_test)\n",
        "print(\"RNN Test Accuracy:\", accuracy)\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(32, input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "loss, accuracy = lstm_model.evaluate(X_test_cnn, y_test)\n",
        "print(\"LSTM Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uvapLPS7Uad",
        "outputId": "7d779d81-927a-40a1-88b0-c8d7f1aa1826"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8270 - loss: 0.3294 - val_accuracy: 0.9984 - val_loss: 0.0148\n",
            "Epoch 2/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9968 - loss: 0.0204 - val_accuracy: 0.9984 - val_loss: 0.0134\n",
            "Epoch 3/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9961 - loss: 0.0240 - val_accuracy: 0.9984 - val_loss: 0.0122\n",
            "Epoch 4/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0170 - val_accuracy: 0.9984 - val_loss: 0.0128\n",
            "Epoch 5/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9981 - loss: 0.0105 - val_accuracy: 0.9984 - val_loss: 0.0130\n",
            "Epoch 6/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9971 - loss: 0.0151 - val_accuracy: 0.9984 - val_loss: 0.0129\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0174\n",
            "RNN Test Accuracy: 0.9968652129173279\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9966 - loss: 0.3064 - val_accuracy: 0.9984 - val_loss: 0.0126\n",
            "Epoch 2/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9971 - loss: 0.0188 - val_accuracy: 0.9984 - val_loss: 0.0079\n",
            "Epoch 3/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0124 - val_accuracy: 0.9984 - val_loss: 0.0022\n",
            "Epoch 4/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9965 - loss: 0.0083 - val_accuracy: 0.9984 - val_loss: 0.0017\n",
            "Epoch 5/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9976 - loss: 0.0092 - val_accuracy: 0.9984 - val_loss: 0.0018\n",
            "Epoch 6/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
            "Epoch 7/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
            "Epoch 8/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9987 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 9/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9996 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.8529e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 5.2246e-04\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0024\n",
            "LSTM Test Accuracy: 0.9993730187416077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NM1Q9OTV7mvk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}